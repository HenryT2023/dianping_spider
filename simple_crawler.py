#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import time
import random
import json
from bs4 import BeautifulSoup
import pymongo
from urllib.parse import urljoin

class SimpleDianpingCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.base_url = "http://www.dianping.com"
        
        # è®¾ç½®è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # Cookieé…ç½®
        self.cookies = {
            '_hc.v': 'e8c70e5d-fb22-43af-2857-ffe83d9f7329.1758043300',
            '_lxsdk': '199538be002c8-0b02bd0c8c3bd3-16525636-708000-199538be002c8',
            '_lxsdk_cuid': '199538be002c8-0b02bd0c8c3bd3-16525636-708000-199538be002c8',
            '_lxsdk_s': '19955ae6eca-ef3-b42-2c7%7C%7C24',
            'fspop': 'test',
            'logan_session_token': 'm0iywvv5uwdsy0t9q92o'
        }
        
        # MongoDBè¿æ¥
        try:
            self.mongo_client = pymongo.MongoClient('mongodb://localhost:27017/')
            self.db = self.mongo_client['dianping']
            self.collection = self.db['restaurants']
            print("âœ… MongoDBè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ MongoDBè¿æ¥å¤±è´¥: {e}")
            self.mongo_client = None
    
    def get_page(self, url, max_retries=3):
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(max_retries):
            try:
                print(f"æ­£åœ¨è¯·æ±‚: {url} (å°è¯• {attempt + 1}/{max_retries})")
                
                # éšæœºå»¶è¿Ÿ
                time.sleep(random.uniform(5, 10))
                
                response = self.session.get(
                    url, 
                    headers=self.headers, 
                    cookies=self.cookies,
                    timeout=15,
                    allow_redirects=True
                )
                
                print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                print(f"å“åº”URL: {response.url}")
                
                if response.status_code == 200:
                    return response
                else:
                    print(f"è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}")
                    
            except Exception as e:
                print(f"è¯·æ±‚å¼‚å¸¸: {e}")
                
            if attempt < max_retries - 1:
                wait_time = random.uniform(10, 20)
                print(f"ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)
        
        return None
    
    def parse_restaurant_info(self, html_content):
        """è§£æé¤å…ä¿¡æ¯"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            restaurants = []
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾åˆ°é¤å…ä¿¡æ¯
            selectors = [
                '.shop-list .shop-item',
                '.shoplist .shopitem',
                '.shop-wrap',
                '.list-item',
                '[data-shopid]'
            ]
            
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    print(f"æ‰¾åˆ° {len(items)} ä¸ªé¤å…é¡¹ç›® (ä½¿ç”¨é€‰æ‹©å™¨: {selector})")
                    
                    for item in items:
                        restaurant = self.extract_restaurant_data(item)
                        if restaurant:
                            restaurants.append(restaurant)
                    break
            
            return restaurants
            
        except Exception as e:
            print(f"è§£æHTMLæ—¶å‡ºé”™: {e}")
            return []
    
    def extract_restaurant_data(self, item):
        """æå–å•ä¸ªé¤å…æ•°æ®"""
        try:
            restaurant = {}
            
            # æå–é¤å…åç§°
            name_selectors = ['.shop-name', '.shopname', 'h3', '.title', 'a[title]']
            for selector in name_selectors:
                name_elem = item.select_one(selector)
                if name_elem:
                    restaurant['name'] = name_elem.get_text(strip=True)
                    break
            
            # æå–è¯„åˆ†
            rating_selectors = ['.shop-star', '.star', '.rating', '[class*="star"]']
            for selector in rating_selectors:
                rating_elem = item.select_one(selector)
                if rating_elem:
                    restaurant['rating'] = rating_elem.get_text(strip=True)
                    break
            
            # æå–åœ°å€
            address_selectors = ['.shop-addr', '.address', '.addr']
            for selector in address_selectors:
                addr_elem = item.select_one(selector)
                if addr_elem:
                    restaurant['address'] = addr_elem.get_text(strip=True)
                    break
            
            # æå–ä»·æ ¼
            price_selectors = ['.shop-price', '.price', '.avgprice']
            for selector in price_selectors:
                price_elem = item.select_one(selector)
                if price_elem:
                    restaurant['price'] = price_elem.get_text(strip=True)
                    break
            
            # æå–åº—é“ºID
            shop_id = item.get('data-shopid') or item.get('data-id')
            if shop_id:
                restaurant['shop_id'] = shop_id
            
            return restaurant if restaurant.get('name') else None
            
        except Exception as e:
            print(f"æå–é¤å…æ•°æ®æ—¶å‡ºé”™: {e}")
            return None
    
    def save_to_mongodb(self, restaurants):
        """ä¿å­˜åˆ°MongoDB"""
        if not self.mongo_client or not restaurants:
            return
        
        try:
            result = self.collection.insert_many(restaurants)
            print(f"âœ… æˆåŠŸä¿å­˜ {len(result.inserted_ids)} æ¡é¤å…æ•°æ®åˆ°MongoDB")
        except Exception as e:
            print(f"âŒ ä¿å­˜åˆ°MongoDBæ—¶å‡ºé”™: {e}")
    
    def crawl_search_page(self, keyword="è‡ªåŠ©é¤", location="å¤§è¿"):
        """çˆ¬å–æœç´¢é¡µé¢"""
        print(f"å¼€å§‹çˆ¬å–: {keyword} - {location}")
        
        # æ„å»ºæœç´¢URL
        search_urls = [
            f"http://www.dianping.com/dalian/ch10/g110",  # å¤§è¿è‡ªåŠ©é¤
            f"http://www.dianping.com/search/keyword/8/10_{keyword}",  # é€šç”¨æœç´¢
            f"http://www.dianping.com/dalian/food",  # å¤§è¿ç¾é£Ÿ
        ]
        
        all_restaurants = []
        
        for url in search_urls:
            print(f"\nå°è¯•URL: {url}")
            response = self.get_page(url)
            
            if response:
                print(f"é¡µé¢å†…å®¹é•¿åº¦: {len(response.text)}")
                
                # ä¿å­˜HTMLç”¨äºè°ƒè¯•
                with open('/Users/hal/Documents/GitHub/dianping_spider/debug_page.html', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                print("é¡µé¢å†…å®¹å·²ä¿å­˜åˆ° debug_page.html")
                
                restaurants = self.parse_restaurant_info(response.text)
                if restaurants:
                    all_restaurants.extend(restaurants)
                    print(f"ä»æ­¤é¡µé¢æå–åˆ° {len(restaurants)} ä¸ªé¤å…")
                    break
                else:
                    print("æœªèƒ½ä»æ­¤é¡µé¢æå–åˆ°é¤å…ä¿¡æ¯")
            else:
                print("æ— æ³•è·å–é¡µé¢å†…å®¹")
        
        if all_restaurants:
            print(f"\næ€»å…±è·å–åˆ° {len(all_restaurants)} ä¸ªé¤å…ä¿¡æ¯:")
            for i, restaurant in enumerate(all_restaurants[:5], 1):  # æ˜¾ç¤ºå‰5ä¸ª
                print(f"{i}. {restaurant}")
            
            self.save_to_mongodb(all_restaurants)
            return all_restaurants
        else:
            print("âŒ æœªèƒ½è·å–åˆ°ä»»ä½•é¤å…æ•°æ®")
            return []

def main():
    print("=== ç®€åŒ–ç‰ˆå¤§ä¼—ç‚¹è¯„çˆ¬è™« ===\n")
    
    crawler = SimpleDianpingCrawler()
    restaurants = crawler.crawl_search_page()
    
    if restaurants:
        print(f"\nğŸ‰ çˆ¬å–æˆåŠŸï¼è·å–åˆ° {len(restaurants)} æ¡é¤å…æ•°æ®")
    else:
        print("\nğŸ˜ çˆ¬å–å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒCookieé…ç½®")

if __name__ == "__main__":
    main()
