#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import time
import random
import json
import re
from bs4 import BeautifulSoup
import pymongo
from urllib.parse import quote
import configparser
from datetime import datetime

class RealDianpingCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.load_config()
        self.setup_mongodb()
        
    def load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        config = configparser.ConfigParser()
        config.read('config.ini', encoding='utf-8')
        
        # è·å–Cookie
        self.cookies_str = config.get('requests', 'cookie', fallback='')
        self.cookies = self.parse_cookies(self.cookies_str)
        
        # è·å–å…¶ä»–é…ç½®
        self.user_agent = config.get('requests', 'user_agent', fallback='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36')
        
        print(f"CookieçŠ¶æ€: {'å·²é…ç½®' if self.cookies else 'æœªé…ç½®'}")
        
    def parse_cookies(self, cookie_str):
        """è§£æCookieå­—ç¬¦ä¸²"""
        if not cookie_str:
            return {}
        
        cookies = {}
        for item in cookie_str.split(';'):
            if '=' in item:
                key, value = item.strip().split('=', 1)
                cookies[key] = value
        return cookies
    
    def setup_mongodb(self):
        """è®¾ç½®MongoDBè¿æ¥"""
        try:
            self.mongo_client = pymongo.MongoClient('mongodb://localhost:27017/')
            self.db = self.mongo_client['dianping']
            self.restaurants_collection = self.db['real_restaurants']
            self.reviews_collection = self.db['real_reviews']
            print("âœ… MongoDBè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ MongoDBè¿æ¥å¤±è´¥: {e}")
            self.mongo_client = None
    
    def get_headers(self):
        """è·å–è¯·æ±‚å¤´"""
        return {
            'User-Agent': self.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://www.dianping.com/',
            'DNT': '1',
        }
    
    def test_connection(self):
        """æµ‹è¯•è¿æ¥å’ŒCookieæœ‰æ•ˆæ€§"""
        print("ğŸ” æµ‹è¯•å¤§ä¼—ç‚¹è¯„è¿æ¥...")
        
        test_urls = [
            'https://www.dianping.com/',
            'https://www.dianping.com/dalian',
            'http://www.dianping.com/dalian/ch10'
        ]
        
        for url in test_urls:
            try:
                print(f"æµ‹è¯•URL: {url}")
                response = self.session.get(
                    url,
                    headers=self.get_headers(),
                    cookies=self.cookies,
                    timeout=10,
                    allow_redirects=True
                )
                
                print(f"çŠ¶æ€ç : {response.status_code}")
                print(f"æœ€ç»ˆURL: {response.url}")
                print(f"å†…å®¹é•¿åº¦: {len(response.text)}")
                
                # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
                if response.status_code == 200:
                    if 'verify' in response.url or 'login' in response.url:
                        print("âš ï¸ è¢«é‡å®šå‘åˆ°éªŒè¯/ç™»å½•é¡µé¢")
                    elif 'å¤§ä¼—ç‚¹è¯„' in response.text or 'dianping' in response.text.lower():
                        print("âœ… è¿æ¥æˆåŠŸï¼")
                        return True
                    else:
                        print("âš ï¸ é¡µé¢å†…å®¹å¼‚å¸¸")
                else:
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {response.status_code}")
                
                time.sleep(2)
                
            except Exception as e:
                print(f"âŒ è¿æ¥é”™è¯¯: {e}")
        
        return False
    
    def manual_cookie_update(self):
        """æ‰‹åŠ¨æ›´æ–°Cookie"""
        print("\n" + "="*60)
        print("ğŸª æ‰‹åŠ¨Cookieæ›´æ–°æŒ‡å—")
        print("="*60)
        print("1. æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—® https://www.dianping.com/dalian")
        print("2. ç™»å½•ä½ çš„å¤§ä¼—ç‚¹è¯„è´¦å·")
        print("3. æŒ‰F12æ‰“å¼€å¼€å‘è€…å·¥å…·")
        print("4. åˆ‡æ¢åˆ°Networkæ ‡ç­¾")
        print("5. åˆ·æ–°é¡µé¢")
        print("6. æ‰¾åˆ°ç¬¬ä¸€ä¸ªè¯·æ±‚ï¼Œå³é”® -> Copy -> Copy as cURL")
        print("7. ä»cURLä¸­æå–Cookieéƒ¨åˆ†")
        print("\nè¯·è¾“å…¥æ–°çš„Cookie (ç›´æ¥ç²˜è´´å®Œæ•´çš„Cookieå­—ç¬¦ä¸²):")
        
        new_cookie = input().strip()
        
        if new_cookie:
            # æ›´æ–°é…ç½®æ–‡ä»¶
            config = configparser.ConfigParser()
            config.read('config.ini', encoding='utf-8')
            config.set('requests', 'cookie', new_cookie)
            
            with open('config.ini', 'w', encoding='utf-8') as f:
                config.write(f)
            
            # é‡æ–°åŠ è½½é…ç½®
            self.load_config()
            print("âœ… Cookieå·²æ›´æ–°ï¼")
            
            # æµ‹è¯•æ–°Cookie
            if self.test_connection():
                return True
        
        return False
    
    def try_different_strategies(self):
        """å°è¯•ä¸åŒçš„çˆ¬å–ç­–ç•¥"""
        print("\nğŸš€ å°è¯•ä¸åŒçš„çˆ¬å–ç­–ç•¥...")
        
        strategies = [
            {
                'name': 'ç­–ç•¥1: ç›´æ¥è®¿é—®é¤å…åˆ—è¡¨',
                'urls': [
                    'https://www.dianping.com/dalian/ch10/g110',
                    'https://www.dianping.com/dalian/ch10',
                ]
            },
            {
                'name': 'ç­–ç•¥2: æœç´¢æ¥å£',
                'urls': [
                    'https://www.dianping.com/search/keyword/8/10_è‡ªåŠ©é¤',
                    'https://www.dianping.com/dalian/search/category/10/10/g110',
                ]
            },
            {
                'name': 'ç­–ç•¥3: AJAXæ¥å£',
                'urls': [
                    'https://www.dianping.com/ajax/json/shop/category/shoplist',
                    'https://www.dianping.com/ajax/json/search/searchshop',
                ]
            }
        ]
        
        for strategy in strategies:
            print(f"\nğŸ”„ {strategy['name']}")
            
            for url in strategy['urls']:
                try:
                    print(f"å°è¯•: {url}")
                    
                    # éšæœºå»¶è¿Ÿ
                    time.sleep(random.uniform(3, 8))
                    
                    response = self.session.get(
                        url,
                        headers=self.get_headers(),
                        cookies=self.cookies,
                        timeout=15,
                        allow_redirects=True
                    )
                    
                    print(f"çŠ¶æ€: {response.status_code} | URL: {response.url}")
                    
                    if response.status_code == 200 and 'verify' not in response.url:
                        # ä¿å­˜é¡µé¢ç”¨äºåˆ†æ
                        filename = f"page_{int(time.time())}.html"
                        with open(filename, 'w', encoding='utf-8') as f:
                            f.write(response.text)
                        
                        print(f"âœ… æˆåŠŸè·å–é¡µé¢ï¼Œå·²ä¿å­˜ä¸º {filename}")
                        
                        # å°è¯•è§£ææ•°æ®
                        restaurants = self.parse_page(response.text)
                        if restaurants:
                            print(f"ğŸ‰ è§£æåˆ° {len(restaurants)} ä¸ªé¤å…ï¼")
                            self.save_restaurants(restaurants)
                            return restaurants
                    
                except Exception as e:
                    print(f"âŒ è¯·æ±‚å¤±è´¥: {e}")
        
        return []
    
    def parse_page(self, html_content):
        """è§£æé¡µé¢å†…å®¹"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            restaurants = []
            
            # å¤šç§è§£æç­–ç•¥
            selectors = [
                '.shop-list .shop-item',
                '.shoplist .shopitem',
                '.shop-wrap',
                '.list-item',
                '[data-shopid]',
                '.shop-info',
                '.poi-item'
            ]
            
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    print(f"ä½¿ç”¨é€‰æ‹©å™¨ {selector} æ‰¾åˆ° {len(items)} ä¸ªé¡¹ç›®")
                    
                    for item in items:
                        restaurant = self.extract_restaurant_info(item)
                        if restaurant:
                            restaurants.append(restaurant)
                    
                    if restaurants:
                        break
            
            # å¦‚æœHTMLè§£æå¤±è´¥ï¼Œå°è¯•JSONæ•°æ®
            if not restaurants:
                restaurants = self.extract_json_data(html_content)
            
            return restaurants
            
        except Exception as e:
            print(f"è§£æé¡µé¢æ—¶å‡ºé”™: {e}")
            return []
    
    def extract_restaurant_info(self, item):
        """æå–é¤å…ä¿¡æ¯"""
        try:
            restaurant = {}
            
            # é¤å…åç§°
            name_elem = item.select_one('.shop-name, .shopname, h3, .title, a[title]')
            if name_elem:
                restaurant['name'] = name_elem.get_text(strip=True)
            
            # è¯„åˆ†
            rating_elem = item.select_one('.shop-star, .star, .rating, .score')
            if rating_elem:
                rating_text = rating_elem.get_text(strip=True)
                rating_match = re.search(r'(\d+\.?\d*)', rating_text)
                if rating_match:
                    restaurant['rating'] = float(rating_match.group(1))
            
            # åœ°å€
            addr_elem = item.select_one('.shop-addr, .address, .addr')
            if addr_elem:
                restaurant['address'] = addr_elem.get_text(strip=True)
            
            # ä»·æ ¼
            price_elem = item.select_one('.shop-price, .price, .avgprice')
            if price_elem:
                price_text = price_elem.get_text(strip=True)
                price_match = re.search(r'(\d+)', price_text)
                if price_match:
                    restaurant['price_per_person'] = int(price_match.group(1))
            
            # åº—é“ºID
            shop_id = item.get('data-shopid') or item.get('data-id')
            if shop_id:
                restaurant['shop_id'] = shop_id
            
            # æ·»åŠ çˆ¬å–æ—¶é—´
            restaurant['crawl_time'] = datetime.now()
            restaurant['data_source'] = 'real_crawl'
            
            return restaurant if restaurant.get('name') else None
            
        except Exception as e:
            print(f"æå–é¤å…ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None
    
    def extract_json_data(self, html_content):
        """ä»é¡µé¢JSONæ•°æ®ä¸­æå–"""
        try:
            # æŸ¥æ‰¾JSONæ•°æ®
            json_patterns = [
                r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
                r'window\.pageConfig\s*=\s*({.+?});',
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, html_content, re.DOTALL)
                if matches:
                    try:
                        data = json.loads(matches[0])
                        restaurants = self.parse_json_restaurants(data)
                        if restaurants:
                            print(f"ä»JSONæå–åˆ° {len(restaurants)} ä¸ªé¤å…")
                            return restaurants
                    except:
                        continue
            
            return []
            
        except Exception as e:
            print(f"æå–JSONæ•°æ®æ—¶å‡ºé”™: {e}")
            return []
    
    def parse_json_restaurants(self, data):
        """è§£æJSONä¸­çš„é¤å…æ•°æ®"""
        restaurants = []
        
        def find_restaurants(obj):
            if isinstance(obj, dict):
                for key, value in obj.items():
                    if key in ['shops', 'list', 'data', 'results', 'items']:
                        if isinstance(value, list):
                            for item in value:
                                if isinstance(item, dict) and 'name' in item:
                                    restaurant = {
                                        'name': item.get('name', ''),
                                        'shop_id': item.get('id', item.get('shopId', '')),
                                        'rating': item.get('rating', item.get('avgRating', 0)),
                                        'address': item.get('address', ''),
                                        'category': item.get('category', ''),
                                        'price_per_person': item.get('avgPrice', 0),
                                        'review_count': item.get('reviewCount', 0),
                                        'crawl_time': datetime.now(),
                                        'data_source': 'real_crawl'
                                    }
                                    restaurants.append(restaurant)
                    else:
                        find_restaurants(value)
            elif isinstance(obj, list):
                for item in obj:
                    find_restaurants(item)
        
        find_restaurants(data)
        return restaurants
    
    def save_restaurants(self, restaurants):
        """ä¿å­˜é¤å…æ•°æ®"""
        if not self.mongo_client or not restaurants:
            return
        
        try:
            # æ¸…ç©ºæ—§æ•°æ®
            self.restaurants_collection.delete_many({'data_source': 'real_crawl'})
            
            # æ’å…¥æ–°æ•°æ®
            result = self.restaurants_collection.insert_many(restaurants)
            print(f"âœ… æˆåŠŸä¿å­˜ {len(result.inserted_ids)} æ¡çœŸå®é¤å…æ•°æ®")
            
            # æ˜¾ç¤ºæ•°æ®
            self.display_restaurants()
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def display_restaurants(self):
        """æ˜¾ç¤ºé¤å…æ•°æ®"""
        print("\n" + "="*60)
        print("ğŸ‰ çœŸå®å¤§ä¼—ç‚¹è¯„é¤å…æ•°æ®")
        print("="*60)
        
        restaurants = list(self.restaurants_collection.find({'data_source': 'real_crawl'}).limit(10))
        
        for i, restaurant in enumerate(restaurants, 1):
            print(f"\n{i}. {restaurant.get('name', 'N/A')}")
            if restaurant.get('rating'):
                print(f"   è¯„åˆ†: {restaurant['rating']} â­")
            if restaurant.get('address'):
                print(f"   åœ°å€: {restaurant['address']}")
            if restaurant.get('price_per_person'):
                print(f"   äººå‡: Â¥{restaurant['price_per_person']}")
            if restaurant.get('category'):
                print(f"   ç±»å‹: {restaurant['category']}")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        print("=== çœŸå®å¤§ä¼—ç‚¹è¯„æ•°æ®çˆ¬è™« ===\n")
        
        # æ£€æŸ¥Cookieé…ç½®
        if not self.cookies:
            print("âš ï¸ æœªæ£€æµ‹åˆ°Cookieé…ç½®")
            if input("æ˜¯å¦éœ€è¦æ‰‹åŠ¨æ›´æ–°Cookie? (y/n): ").lower() == 'y':
                if not self.manual_cookie_update():
                    print("âŒ Cookieæ›´æ–°å¤±è´¥ï¼Œæ— æ³•ç»§ç»­")
                    return
        
        # æµ‹è¯•è¿æ¥
        if not self.test_connection():
            print("âŒ è¿æ¥æµ‹è¯•å¤±è´¥")
            if input("æ˜¯å¦éœ€è¦æ›´æ–°Cookie? (y/n): ").lower() == 'y':
                if not self.manual_cookie_update():
                    print("âŒ æ— æ³•å»ºç«‹æœ‰æ•ˆè¿æ¥")
                    return
        
        # å¼€å§‹çˆ¬å–
        restaurants = self.try_different_strategies()
        
        if restaurants:
            print(f"\nğŸ‰ æˆåŠŸè·å– {len(restaurants)} æ¡çœŸå®æ•°æ®ï¼")
        else:
            print("\nğŸ˜ æš‚æ—¶æ— æ³•è·å–çœŸå®æ•°æ®")
            print("å»ºè®®:")
            print("1. ç¡®ä¿ç½‘ç»œè¿æ¥æ­£å¸¸")
            print("2. æ›´æ–°Cookieé…ç½®")
            print("3. å°è¯•ä½¿ç”¨ä»£ç†æœåŠ¡")

if __name__ == "__main__":
    crawler = RealDianpingCrawler()
    crawler.run()
