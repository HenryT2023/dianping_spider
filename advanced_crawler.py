#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import requests
import time
import random
import json
import re
from bs4 import BeautifulSoup
import pymongo
from urllib.parse import urljoin, quote
import base64

class AdvancedDianpingCrawler:
    def __init__(self):
        self.session = requests.Session()
        
        # é¢„å®šä¹‰çš„User-Agentåˆ—è¡¨
        self.user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:121.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.2 Safari/605.1.15'
        ]
        
        # æ›´æ–°çš„Cookieé…ç½®
        self.cookies = {
            '_hc.v': 'e8c70e5d-fb22-43af-2857-ffe83d9f7329.1758043300',
            '_lxsdk': '199538be002c8-0b02bd0c8c3bd3-16525636-708000-199538be002c8',
            '_lxsdk_cuid': '199538be002c8-0b02bd0c8c3bd3-16525636-708000-199538be002c8',
            '_lxsdk_s': '19955ae6eca-ef3-b42-2c7%7C%7C24',
            'fspop': 'test',
            'logan_session_token': 'm0iywvv5uwdsy0t9q92o',
            # æ·»åŠ æ›´å¤šå¿…è¦çš„Cookie
            'cy': '8',  # åŸå¸‚ä»£ç 
            'cye': 'dalian',  # åŸå¸‚å
        }
        
        # MongoDBè¿æ¥
        try:
            self.mongo_client = pymongo.MongoClient('mongodb://localhost:27017/')
            self.db = self.mongo_client['dianping']
            self.restaurants_collection = self.db['real_restaurants']
            self.reviews_collection = self.db['real_reviews']
            print("âœ… MongoDBè¿æ¥æˆåŠŸ")
        except Exception as e:
            print(f"âŒ MongoDBè¿æ¥å¤±è´¥: {e}")
            self.mongo_client = None
    
    def get_random_headers(self):
        """ç”Ÿæˆéšæœºè¯·æ±‚å¤´"""
        return {
            'User-Agent': random.choice(self.user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
        }
    
    def smart_request(self, url, max_retries=3):
        """æ™ºèƒ½è¯·æ±‚æ–¹æ³•"""
        for attempt in range(max_retries):
            try:
                headers = self.get_random_headers()
                
                # éšæœºå»¶è¿Ÿ
                delay = random.uniform(8, 15)
                print(f"ç­‰å¾… {delay:.1f} ç§’...")
                time.sleep(delay)
                
                print(f"æ­£åœ¨è¯·æ±‚: {url} (å°è¯• {attempt + 1}/{max_retries})")
                
                response = self.session.get(
                    url,
                    headers=headers,
                    cookies=self.cookies,
                    timeout=20,
                    allow_redirects=True
                )
                
                print(f"å“åº”çŠ¶æ€ç : {response.status_code}")
                print(f"å“åº”URL: {response.url}")
                print(f"å†…å®¹é•¿åº¦: {len(response.text)}")
                
                # æ£€æŸ¥æ˜¯å¦è¢«é‡å®šå‘åˆ°éªŒè¯é¡µé¢
                if 'verify' in response.url or 'login' in response.url:
                    print("âš ï¸ é‡åˆ°éªŒè¯é¡µé¢ï¼Œå°è¯•å…¶ä»–ç­–ç•¥...")
                    continue
                
                if response.status_code == 200:
                    return response
                    
            except Exception as e:
                print(f"è¯·æ±‚å¼‚å¸¸: {e}")
                
            if attempt < max_retries - 1:
                wait_time = random.uniform(15, 30)
                print(f"ç­‰å¾… {wait_time:.1f} ç§’åé‡è¯•...")
                time.sleep(wait_time)
        
        return None
    
    def try_mobile_api(self, keyword="è‡ªåŠ©é¤", city_id=8):
        """å°è¯•ç§»åŠ¨ç«¯API"""
        mobile_urls = [
            f"https://m.dianping.com/search/keyword/{city_id}/0_{quote(keyword)}",
            f"https://m.dianping.com/{city_id}/food",
            f"https://m.dianping.com/dalian/food/buffet",
        ]
        
        for url in mobile_urls:
            print(f"\nğŸ”„ å°è¯•ç§»åŠ¨ç«¯URL: {url}")
            
            # æ¨¡æ‹Ÿç§»åŠ¨ç«¯è¯·æ±‚å¤´
            mobile_headers = {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9',
                'Accept-Encoding': 'gzip, deflate, br',
                'Connection': 'keep-alive',
            }
            
            try:
                response = self.session.get(
                    url,
                    headers=mobile_headers,
                    cookies=self.cookies,
                    timeout=15
                )
                
                if response.status_code == 200 and 'verify' not in response.url:
                    print(f"âœ… ç§»åŠ¨ç«¯è¯·æ±‚æˆåŠŸ")
                    return response
                    
            except Exception as e:
                print(f"ç§»åŠ¨ç«¯è¯·æ±‚å¤±è´¥: {e}")
        
        return None
    
    def parse_restaurant_list(self, html_content):
        """è§£æé¤å…åˆ—è¡¨"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            restaurants = []
            
            # å¤šç§é€‰æ‹©å™¨ç­–ç•¥
            selectors = [
                '.shop-list .shop-item',
                '.shoplist .shopitem', 
                '.shop-wrap',
                '.list-item',
                '[data-shopid]',
                '.shop-info',
                '.poi-item',
                '.shop-card'
            ]
            
            for selector in selectors:
                items = soup.select(selector)
                if items:
                    print(f"æ‰¾åˆ° {len(items)} ä¸ªé¤å…é¡¹ç›® (é€‰æ‹©å™¨: {selector})")
                    
                    for item in items:
                        restaurant = self.extract_restaurant_info(item)
                        if restaurant:
                            restaurants.append(restaurant)
                    
                    if restaurants:
                        break
            
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•ä»JSONæ•°æ®ä¸­æå–
            if not restaurants:
                restaurants = self.extract_from_json(html_content)
            
            return restaurants
            
        except Exception as e:
            print(f"è§£æé¤å…åˆ—è¡¨æ—¶å‡ºé”™: {e}")
            return []
    
    def extract_from_json(self, html_content):
        """ä»é¡µé¢JSONæ•°æ®ä¸­æå–ä¿¡æ¯"""
        try:
            # æŸ¥æ‰¾é¡µé¢ä¸­çš„JSONæ•°æ®
            json_patterns = [
                r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
                r'window\.pageConfig\s*=\s*({.+?});',
                r'__INITIAL_DATA__\s*=\s*({.+?});'
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, html_content, re.DOTALL)
                if matches:
                    try:
                        data = json.loads(matches[0])
                        restaurants = self.parse_json_data(data)
                        if restaurants:
                            print(f"ä»JSONæ•°æ®ä¸­æå–åˆ° {len(restaurants)} ä¸ªé¤å…")
                            return restaurants
                    except:
                        continue
            
            return []
            
        except Exception as e:
            print(f"ä»JSONæå–æ•°æ®æ—¶å‡ºé”™: {e}")
            return []
    
    def parse_json_data(self, data):
        """è§£æJSONæ•°æ®"""
        restaurants = []
        
        def find_restaurants(obj):
            if isinstance(obj, dict):
                # æŸ¥æ‰¾å¯èƒ½åŒ…å«é¤å…ä¿¡æ¯çš„å­—æ®µ
                for key, value in obj.items():
                    if key in ['shops', 'list', 'data', 'results', 'items']:
                        if isinstance(value, list):
                            for item in value:
                                if isinstance(item, dict) and 'name' in item:
                                    restaurant = {
                                        'name': item.get('name', ''),
                                        'shop_id': item.get('id', item.get('shopId', '')),
                                        'rating': item.get('rating', item.get('avgRating', 0)),
                                        'address': item.get('address', ''),
                                        'category': item.get('category', item.get('categoryName', '')),
                                        'price': item.get('avgPrice', item.get('price', 0)),
                                        'review_count': item.get('reviewCount', 0)
                                    }
                                    restaurants.append(restaurant)
                    else:
                        find_restaurants(value)
            elif isinstance(obj, list):
                for item in obj:
                    find_restaurants(item)
        
        find_restaurants(data)
        return restaurants
    
    def extract_restaurant_info(self, item):
        """æå–å•ä¸ªé¤å…ä¿¡æ¯"""
        try:
            restaurant = {}
            
            # é¤å…åç§°
            name_selectors = ['.shop-name', '.shopname', 'h3', '.title', 'a[title]', '.poi-name']
            for selector in name_selectors:
                elem = item.select_one(selector)
                if elem:
                    restaurant['name'] = elem.get_text(strip=True)
                    break
            
            # è¯„åˆ†
            rating_selectors = ['.shop-star', '.star', '.rating', '[class*="star"]', '.score']
            for selector in rating_selectors:
                elem = item.select_one(selector)
                if elem:
                    rating_text = elem.get_text(strip=True)
                    rating_match = re.search(r'(\d+\.?\d*)', rating_text)
                    if rating_match:
                        restaurant['rating'] = float(rating_match.group(1))
                    break
            
            # åœ°å€
            address_selectors = ['.shop-addr', '.address', '.addr', '.location']
            for selector in address_selectors:
                elem = item.select_one(selector)
                if elem:
                    restaurant['address'] = elem.get_text(strip=True)
                    break
            
            # ä»·æ ¼
            price_selectors = ['.shop-price', '.price', '.avgprice', '.per-price']
            for selector in price_selectors:
                elem = item.select_one(selector)
                if elem:
                    price_text = elem.get_text(strip=True)
                    price_match = re.search(r'(\d+)', price_text)
                    if price_match:
                        restaurant['price_per_person'] = int(price_match.group(1))
                    break
            
            # åº—é“ºID
            shop_id = item.get('data-shopid') or item.get('data-id')
            if shop_id:
                restaurant['shop_id'] = shop_id
            
            return restaurant if restaurant.get('name') else None
            
        except Exception as e:
            print(f"æå–é¤å…ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return None
    
    def crawl_real_data(self):
        """çˆ¬å–çœŸå®æ•°æ®"""
        print("ğŸš€ å¼€å§‹çˆ¬å–çœŸå®çš„å¤§ä¼—ç‚¹è¯„æ•°æ®...")
        
        all_restaurants = []
        
        # ç­–ç•¥1: å°è¯•ä¸åŒçš„æœç´¢URL
        search_urls = [
            "http://www.dianping.com/dalian/ch10/g110",  # å¤§è¿è‡ªåŠ©é¤
            "http://www.dianping.com/dalian/ch10",       # å¤§è¿ç¾é£Ÿ
            "http://www.dianping.com/search/keyword/8/10_è‡ªåŠ©é¤",
        ]
        
        for url in search_urls:
            print(f"\nğŸ”„ å°è¯•URL: {url}")
            response = self.smart_request(url)
            
            if response:
                # ä¿å­˜é¡µé¢ç”¨äºè°ƒè¯•
                with open('/Users/hal/Documents/GitHub/dianping_spider/real_page.html', 'w', encoding='utf-8') as f:
                    f.write(response.text)
                
                restaurants = self.parse_restaurant_list(response.text)
                if restaurants:
                    all_restaurants.extend(restaurants)
                    print(f"âœ… ä»æ­¤URLè·å–åˆ° {len(restaurants)} ä¸ªé¤å…")
                    break
        
        # ç­–ç•¥2: å°è¯•ç§»åŠ¨ç«¯API
        if not all_restaurants:
            print("\nğŸ”„ å°è¯•ç§»åŠ¨ç«¯API...")
            mobile_response = self.try_mobile_api()
            if mobile_response:
                restaurants = self.parse_restaurant_list(mobile_response.text)
                if restaurants:
                    all_restaurants.extend(restaurants)
        
        # ä¿å­˜æ•°æ®
        if all_restaurants:
            self.save_real_data(all_restaurants)
            return all_restaurants
        else:
            print("âŒ æœªèƒ½è·å–åˆ°çœŸå®æ•°æ®")
            return []
    
    def save_real_data(self, restaurants):
        """ä¿å­˜çœŸå®æ•°æ®"""
        if not self.mongo_client or not restaurants:
            return
        
        try:
            # æ¸…ç©ºä¹‹å‰çš„æ•°æ®
            self.restaurants_collection.delete_many({})
            
            # æ·»åŠ çˆ¬å–æ—¶é—´
            for restaurant in restaurants:
                restaurant['crawl_time'] = time.time()
                restaurant['data_source'] = 'real_crawl'
            
            result = self.restaurants_collection.insert_many(restaurants)
            print(f"âœ… æˆåŠŸä¿å­˜ {len(result.inserted_ids)} æ¡çœŸå®é¤å…æ•°æ®")
            
            # å±•ç¤ºæ•°æ®
            self.display_real_data()
            
        except Exception as e:
            print(f"âŒ ä¿å­˜çœŸå®æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def display_real_data(self):
        """å±•ç¤ºçœŸå®æ•°æ®"""
        print("\n" + "="*60)
        print("ğŸ‰ çœŸå®å¤§ä¼—ç‚¹è¯„æ•°æ®")
        print("="*60)
        
        restaurants = list(self.restaurants_collection.find().limit(10))
        for i, restaurant in enumerate(restaurants, 1):
            print(f"\n{i}. é¤å…: {restaurant.get('name', 'N/A')}")
            if restaurant.get('rating'):
                print(f"   è¯„åˆ†: {restaurant['rating']} â­")
            if restaurant.get('address'):
                print(f"   åœ°å€: {restaurant['address']}")
            if restaurant.get('price_per_person'):
                print(f"   äººå‡: Â¥{restaurant['price_per_person']}")
            if restaurant.get('category'):
                print(f"   ç±»å‹: {restaurant['category']}")

def main():
    print("=== é«˜çº§å¤§ä¼—ç‚¹è¯„çˆ¬è™« ===")
    print("ä¸“æ³¨è·å–çœŸå®ç‚¹è¯„æ•°æ®\n")
    
    crawler = AdvancedDianpingCrawler()
    restaurants = crawler.crawl_real_data()
    
    if restaurants:
        print(f"\nğŸ‰ æˆåŠŸè·å– {len(restaurants)} æ¡çœŸå®æ•°æ®ï¼")
    else:
        print("\nğŸ˜ æš‚æ—¶æ— æ³•çªç ´åçˆ¬é™åˆ¶ï¼Œå»ºè®®:")
        print("1. æ›´æ–°Cookieé…ç½®")
        print("2. ä½¿ç”¨ä»£ç†æœåŠ¡")
        print("3. é™ä½è¯·æ±‚é¢‘ç‡")

if __name__ == "__main__":
    main()
